{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chapter_2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMvTmH5+lF04CIlbyl8Y4o2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NbB9P_GgtXMB"},"source":["Python programmers already familiar with ndarray from NumPy package.\n","Tensor in PyTorch is similar to NumPy’s ndarray with two key additions\n","Support for computation on GPUs, Automatic differentiation.\n","These features are critical for deep learning. \n","\n","A tensor represents a array of numerical values. With one axis, a tensor corresponds to a vector.\n","With two axes, a tensor corresponds to a matrix. \n","Tensors with more than two axes do not have special mathematical names.\n","\n","This notebook will walk you through the basics of working with tensors in PyTorch. \n","\n","The first thing to do is to import pytorch. "]},{"cell_type":"code","metadata":{"id":"yiB-j18otDx0","executionInfo":{"status":"ok","timestamp":1619475301371,"user_tz":-120,"elapsed":4816,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}}},"source":["import torch"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xevMZulvTXSQ"},"source":["# Tensor Creation\n","\n","We can use arange to create a row vector x containing the first 12 integers."]},{"cell_type":"code","metadata":{"id":"PSSirP-StmEQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475304155,"user_tz":-120,"elapsed":978,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"9daa1b1a-63bf-48ed-a0ff-cc3ce7708335"},"source":["x = torch.arange(12)\n","x"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"yRP1w5KKrXYo"},"source":["They are created as floats by default.\n","Each of the values in a tensor is called an element of the tensor. \n","Unless specified, a new tensor will be designated for CPU computation.\n","\n","Typically, we want our matrices initialized either with zeros, ones, or other constants.\n","We can create a tensor representing a tensor with all elements set to 0 and a shape of (2, 3, 4) as follows:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_aGkonZSVx9","executionInfo":{"status":"ok","timestamp":1619475306770,"user_tz":-120,"elapsed":571,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"aca43ec1-9e5f-4df7-93ee-2f0ba8866450"},"source":["torch.zeros((2, 3, 4))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]])"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"YXO0tRSIri3U"},"source":["Similarly, we can create tensors with each element set to 1 as follows:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7R3qoZq8SQHq","executionInfo":{"status":"ok","timestamp":1619475309087,"user_tz":-120,"elapsed":556,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"090876e5-32ba-4531-f6f0-d13d3cc53c74"},"source":["torch.ones((2, 3, 4))"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]],\n","\n","        [[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]]])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"H4XKkDjprnL4"},"source":["Often we want to randomly sample the values for each element in a tensor from some probability distribution.\n","For example, we typically initialize parameters in a neural network randomly."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QlKhJjOiSbt_","executionInfo":{"status":"ok","timestamp":1619475311416,"user_tz":-120,"elapsed":699,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"206c1c9c-1961-432d-ca22-bf3d4682a5d4"},"source":["torch.randn(3, 4)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.4754, -2.3061,  1.0743,  1.6016],\n","        [-0.1406, -0.7777,  0.0898, -0.3613],\n","        [ 0.3359, -0.0960,  1.0118, -0.1215]])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"KpWEbI4rrpRm"},"source":["Each element is randomly sampled from a standard Gaussian distribution with a mean of 0 and a standard deviation of 1.\n","\n","We can also specify the exact values for each element by supplying a Python list (or list of lists)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_lyd0bwnSgMi","executionInfo":{"status":"ok","timestamp":1619475313637,"user_tz":-120,"elapsed":581,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"c180a799-1776-4e9e-889e-182c58ae7310"},"source":["torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2, 1, 4, 3],\n","        [1, 2, 3, 4],\n","        [4, 3, 2, 1]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"ALwCaA0MruWm"},"source":["Here, the outermost list corresponds to axis 0, and the inner list to axis 1.\n","\n","One of the most important features of PyTorch is that it can use graphics processing units (GPUs) to accelerate its tensor operations. We can easily check whether PyTorch is configured to use GPUs:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJZZ-rJYllX6","executionInfo":{"status":"ok","timestamp":1619475316327,"user_tz":-120,"elapsed":578,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"c1e43350-f907-4f6c-8e50-5af9c8350031"},"source":["if torch.cuda.is_available:\n","  print('PyTorch can use GPUs!')\n","else:\n","  print('PyTorch cannot use GPUs.')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["PyTorch can use GPUs!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ASrFcbezlvy4"},"source":["Can enable GPUs in Colab via Runtime ->￼ Change Runtime Type ->￼ Hardware Accelerator ->￼ GPU.\n","\n","PyTorch tensors have a device attribute specifying where the tensor is stored.\n","Either CPU, or CUDA for NVIDA GPUs."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JHwxRVNgmNHo","executionInfo":{"status":"ok","timestamp":1619475318569,"user_tz":-120,"elapsed":519,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"8af12487-e415-4284-f32f-0bf590079b2e"},"source":["# Construct a tensor on the CPU\n","x0 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n","print('x0 device:', x0.device)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["x0 device: cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NzeNUWBfmVie"},"source":["Tensor construction on GPU:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eAVsb4G4mYxF","executionInfo":{"status":"ok","timestamp":1619475331408,"user_tz":-120,"elapsed":11051,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"3b8b77d3-351e-4b78-e71e-67734788a794"},"source":["y = torch.tensor([[1, 2, 3], [4, 5, 6]], device='cuda')\n","print('y device:', y.device)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["y device: cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SH4nWNxqnx75"},"source":["We can also use the methods .cuda() and .cpu() methods to move tensors between CPU and GPU.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Div0UCMjn28l","executionInfo":{"status":"ok","timestamp":1619475333709,"user_tz":-120,"elapsed":561,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"2ad31c76-3fd4-4346-dc51-344b2d520551"},"source":["# Move it to the GPU using .cuda()\n","x1 = x0.cuda()\n","print('x1 device:', x1.device)\n","\n","# Move it back to the CPU using .cpu()\n","x2 = x1.cpu()\n","print('x2 device:', x2.device)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["x1 device: cuda:0\n","x2 device: cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5wArw4Imo3sK"},"source":["# Tensor Datatypes\n","\n","PyTorch provides a set of numeric datatypes for tensors.\n","* torch.float32 or torch.float: 32-bit floating-point\n","* torch.float64 or torch.double: 64-bit, double-precision floating-point \n","* torch.float16 or torch.half: 16-bit, half-precision floating-point\n","* torch.int8: signed 8-bit integers\n","* torch.uint8: unsigned 8-bit integers\n","* torch.int16 or torch.short: signed 16-bit integers\n","* torch.int32 or torch.int: signed 32-bit integers\n","* torch.int64 or torch.long: signed 64-bit integers\n","\n","In the examples above, you may have noticed that some of our tensors contained floating-point values, while others contained integer values.\n","PyTorch tries to guess a datatype when you create a tensor."]},{"cell_type":"code","metadata":{"id":"8MZ8KbFDqNSk","executionInfo":{"status":"ok","timestamp":1619475337671,"user_tz":-120,"elapsed":572,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}}},"source":["# Let torch choose the datatype\n","x0 = torch.tensor([1, 2])   # List of integers\n","x1 = torch.tensor([1., 2.]) # List of floats\n","x2 = torch.tensor([1., 2])  # Mixed list"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cjIkPpM4qWpB"},"source":["Each tensor has a dtype attribute that you can use to check its data type:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jtvquhKqZ_Y","executionInfo":{"status":"ok","timestamp":1619475339821,"user_tz":-120,"elapsed":465,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"46e089a0-6188-4f8d-8198-a6861bb4585b"},"source":["print('List of integers:', x0.dtype)\n","print('List of floats:', x1.dtype)\n","print('Mixed list:', x2.dtype)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["List of integers: torch.int64\n","List of floats: torch.float32\n","Mixed list: torch.float32\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vzALQ9eBqfAp"},"source":["Functions that construct tensors typically have a dtype argument that you can use to explicitly specify a datatype."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SRIdwvHVqlT4","executionInfo":{"status":"ok","timestamp":1619475342050,"user_tz":-120,"elapsed":341,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"0e0f30f1-1010-4cf6-9121-f00a6e97de08"},"source":["y0 = torch.tensor([1, 2], dtype=torch.float32)  # 32-bit float\n","y1 = torch.tensor([1, 2], dtype=torch.int32)    # 32-bit integer\n","print('32-bit float: ', y0.dtype)\n","print('32-bit integer: ', y1.dtype)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["32-bit float:  torch.float32\n","32-bit integer:  torch.int32\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jz5dCUJzqxxf"},"source":["We can cast a tensor to another datatype using the .to() method."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ai5OKtFdqsxS","executionInfo":{"status":"ok","timestamp":1619475344321,"user_tz":-120,"elapsed":504,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"0d57158e-4ac1-4bc2-a1b9-6cdd1084af87"},"source":["x0 = torch.ones(1, 2, dtype=torch.int16)\n","x1 = x0.to(torch.float32)\n","x2 = x0.to(torch.float64)\n","print('x0:', x0.dtype)\n","print('x1:', x1.dtype)\n","print('x2:', x2.dtype)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["x0: torch.int16\n","x1: torch.float32\n","x2: torch.float64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GOlrvEfOTflR"},"source":["# Tensor Reshaping\n","\n","We can access a tensor’s shape (the length along each axis) by inspecting its shape property."]},{"cell_type":"code","metadata":{"id":"HGd-jwmeSpPK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475347134,"user_tz":-120,"elapsed":529,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"29553784-22ff-495a-8e0d-811f0e47f0ea"},"source":["x = torch.arange(12)\n","x.shape"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([12])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"3Mj59Cyv2rRR"},"source":["numel function outputs the total number of elements in a tensor."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smlLJirLSwYO","executionInfo":{"status":"ok","timestamp":1619475349697,"user_tz":-120,"elapsed":507,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"462cbf65-e1ef-4ed7-efab-b33f08440573"},"source":["x.numel()"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"XjqQVREF2x1l"},"source":["To change the shape of a tensor without altering either the number of elements or their values, we can invoke the reshape function."]},{"cell_type":"code","metadata":{"id":"uBNQzSSBSy5b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475352396,"user_tz":-120,"elapsed":377,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"9a7b749a-9259-4c36-9e6a-4f1bf231ac4a"},"source":["X = x.reshape(3, 4)\n","X"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"wPsE3I6623pR"},"source":["New tensor contains the same values as a matrix of 3 rows & 4 columns.\n","\n","Reshaping by manually specifying every dimension is unnecessary.\n","Tensors can automatically work out one dimension given the rest. \n","We invoke this by placing -1 for the dimension that we would like tensors to automatically infer.\n","Instead of calling x.reshape(3, 4), we could have equivalently called x.reshape(-1, 4) or x.reshape(3, -1)."]},{"cell_type":"code","metadata":{"id":"rb_Pz_IbTHbP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475354879,"user_tz":-120,"elapsed":376,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"8144185a-7086-40ab-eb0f-2468e015a130"},"source":["x.reshape(-1, 4)\n","X"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"Q3WdXezLTI-x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475357095,"user_tz":-120,"elapsed":357,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"fd5a9887-3346-42d6-f2f5-128c71e8ac94"},"source":["x.reshape(3, -1)\n","X"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]])"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"wl-5Ccek3Jm_"},"source":["Another common reshape operation you might want to perform is transposing a matrix. \n","The reshape() function takes elements in row-major order, so you cannot transpose matrices with .reshape(). "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xxdpaRQT3JHD","executionInfo":{"status":"ok","timestamp":1619475359816,"user_tz":-120,"elapsed":776,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"5bfb0a12-dfa0-4019-e751-bc8fe78d7a67"},"source":["x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","print(x)\n","print(x.reshape(3, 2))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","tensor([[1, 2],\n","        [3, 4],\n","        [5, 6]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lk5lx5gs3Y_r"},"source":["The simplest function to swap axes of a tensor is .t(), specificially for transposing matrices."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGnFsBBm3i7k","executionInfo":{"status":"ok","timestamp":1619475362623,"user_tz":-120,"elapsed":517,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"50d9a637-597e-446c-e084-0a78ae57c7f4"},"source":["print(x)\n","print(x.t())"],"execution_count":21,"outputs":[{"output_type":"stream","text":["tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","tensor([[1, 4],\n","        [2, 5],\n","        [3, 6]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pi-BkxA33uS_"},"source":["For tensors with more than two dimensions, or the .permute() method to arbitrarily permute dimensions."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9kvHYpr3tuI","executionInfo":{"status":"ok","timestamp":1619475365057,"user_tz":-120,"elapsed":373,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"a7e6fe6a-6963-4efb-d3e0-df769a4c1d3f"},"source":["# Create a tensor of shape (2, 3, 4)\n","x0 = torch.tensor([\n","     [[1,  2,  3,  4],\n","      [5,  6,  7,  8],\n","      [9, 10, 11, 12]],\n","     [[13, 14, 15, 16],\n","      [17, 18, 19, 20],\n","      [21, 22, 23, 24]]])\n","print('shape:', x0.shape)\n","x1 = x0.permute(1, 2, 0)\n","print('shape:', x1.shape)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["shape: torch.Size([2, 3, 4])\n","shape: torch.Size([3, 4, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZRqWCmEfTllw"},"source":["# Tensor Elementwise Operations\n","\n","Some of the most useful operations are the elementwise operations. \n","These apply a standard scalar operation to each element of an array. "]},{"cell_type":"code","metadata":{"id":"mEIXCyoeTS3j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475369180,"user_tz":-120,"elapsed":526,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"d59bd587-2744-4193-b186-910517bb36d4"},"source":["x = torch.tensor([1.0, 2, 4, 8])\n","torch.exp(x)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"1RCI-ND86xfk"},"source":["For functions that take two arrays as inputs, elementwise operations apply on each pair of corresponding elements from the two arrays."]},{"cell_type":"code","metadata":{"id":"w9TwPK4pTxUs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475371901,"user_tz":-120,"elapsed":523,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"d7a14b95-913b-431d-fcac-e98fffbaad49"},"source":["x = torch.tensor([1.0, 2, 4, 8])\n","y = torch.tensor([2, 2, 2, 2])\n","x + y, x - y, x * y, x / y, x**y  # The ** operator is exponentiation"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 3.,  4.,  6., 10.]),\n"," tensor([-1.,  0.,  2.,  6.]),\n"," tensor([ 2.,  4.,  8., 16.]),\n"," tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n"," tensor([ 1.,  4., 16., 64.]))"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"CF_PFxXF7WRB"},"source":["We can construct a binary tensor via logical statements. \n","\n","If X and Y are equal at a position, the corresponding entry in the new tensor takes a value of 1; otherwise that position takes 0."]},{"cell_type":"code","metadata":{"id":"ZOEKcHdCT3rb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475374147,"user_tz":-120,"elapsed":363,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"f8cbf2c5-8cfa-40bb-92ff-5d94503bc82f"},"source":["X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n","Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n","\n","X == Y"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[False,  True, False,  True],\n","        [False, False, False, False],\n","        [False, False, False, False]])"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"Hb3eCooRT6LQ"},"source":["# Tensor Broadcasting\n","\n","We performed elementwise operations on two tensors of the same shape.\n","We can perform elementwise operations even when shapes differ.\n","The broadcasting mechanism works in the following way: \n","Expands one or both arrays by copying elements so that the two tensors have the same shape. \n","Carries out the elementwise operations on the resulting arrays.\n","\n","In most cases, we broadcast along an axis where an array has length 1.\n"]},{"cell_type":"code","metadata":{"id":"aunqoaQQT5oS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475377751,"user_tz":-120,"elapsed":563,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"c0098daa-3612-4e23-9a05-0380ee600667"},"source":["a = torch.arange(3).reshape((3, 1))\n","b = torch.arange(2).reshape((1, 2))\n","a, b"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0],\n","         [1],\n","         [2]]), tensor([[0, 1]]))"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"7FUpXRW18f_-"},"source":["Shapes of matrices a, b: 3 X 1 and 1 X 2 respectively do not match up.\n","\n","For elementwise operations PyTorch broadcasts both matrices into a larger 3 X 2 matrix as follows:"]},{"cell_type":"code","metadata":{"id":"_uYSkUJhUDIX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475379770,"user_tz":-120,"elapsed":506,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"6718cd57-a1bb-4d23-d9f9-095189e604cd"},"source":["a + b"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1],\n","        [1, 2],\n","        [2, 3]])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"unbRehS-8pwN"},"source":["For matrix a it replicates the columns and for matrix b it replicates the rows."]},{"cell_type":"markdown","metadata":{"id":"kJgYZ4rsUJnN"},"source":["# Tensor Indexing and Slicing"]},{"cell_type":"markdown","metadata":{"id":"ktqHAwT09pq5"},"source":["Accessing elements works like Python array with first element at index 0.\n","\n","We can use negative indices to access elements according to their position to the end of the list."]},{"cell_type":"code","metadata":{"id":"TtMS8oDLUIx-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475382564,"user_tz":-120,"elapsed":530,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"3c838906-8db9-4a97-cbcc-b9568c5d199a"},"source":["X[-1], X[1:3]"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 8.,  9., 10., 11.]), tensor([[ 4.,  5.,  6.,  7.],\n","         [ 8.,  9., 10., 11.]]))"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"2PONohdJ9wfy"},"source":["[-1] selects the last element and [1:3] selects the second, third elements.\n","\n","We can also write elements of a matrix by specifying indices."]},{"cell_type":"code","metadata":{"id":"bHcLvab6UROz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475384942,"user_tz":-120,"elapsed":361,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"d31d5e99-7e89-4f3e-8dea-0282f0f13c06"},"source":["X[1, 2] = 9\n","X"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  9.,  7.],\n","        [ 8.,  9., 10., 11.]])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"jY0fvNKN-5Vj"},"source":["We can assign multiple elements the same value using indexing."]},{"cell_type":"code","metadata":{"id":"pZC5CQ7CUXzC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475387011,"user_tz":-120,"elapsed":367,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"85d5c8ce-8d18-487d-d539-f908f5bdeb64"},"source":["X[0:2, :] = 12\n","X"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[12., 12., 12., 12.],\n","        [12., 12., 12., 12.],\n","        [ 8.,  9., 10., 11.]])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"ke0eyCUY-9aU"},"source":["[0:2,  :] accesses the first and second rows, where : takes all the elements along column axis."]},{"cell_type":"markdown","metadata":{"id":"M3g0zULy7sRb"},"source":["# Tensor Concatenation\n","\n","We can also stack multiple tensors together to form a larger tensor."]},{"cell_type":"code","metadata":{"id":"j3Y2eBTyT0bM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475390537,"user_tz":-120,"elapsed":569,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"6157c9fd-1cd3-486c-cfde-0d601f96c782"},"source":["X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n","Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n","torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.,  1.,  2.,  3.],\n","         [ 4.,  5.,  6.,  7.],\n","         [ 8.,  9., 10., 11.],\n","         [ 2.,  1.,  4.,  3.],\n","         [ 1.,  2.,  3.,  4.],\n","         [ 4.,  3.,  2.,  1.]]),\n"," tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n","         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n","         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"x0AOvgyTUa6F"},"source":["# Tensor Reduction"]},{"cell_type":"markdown","metadata":{"id":"ATJE38UC_L3C"},"source":["We can calculate the sum of all elements of PyTorch tensors."]},{"cell_type":"code","metadata":{"id":"Ecd0wPl_UjQa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475394168,"user_tz":-120,"elapsed":519,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"41cdc92a-3c38-46d8-a9e8-57674dae6a0e"},"source":["x = torch.arange(4, dtype=torch.float32)\n","x, x.sum()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0., 1., 2., 3.]), tensor(6.))"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"SssWG3Ry_Oy6"},"source":["We can express sums over the elements of tensors of arbitrary shape. "]},{"cell_type":"code","metadata":{"id":"BhbIqRcoUoBg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475396611,"user_tz":-120,"elapsed":556,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"82f353d4-e957-4a08-e937-9a5fa85cb108"},"source":["A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","A.shape, A.sum()"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([5, 4]), tensor(190.))"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"e4khJDMF_VLr"},"source":["We specify axis=0 to reduce the row dimension & sum up row elements."]},{"cell_type":"code","metadata":{"id":"2PACguTgUsH4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475399071,"user_tz":-120,"elapsed":563,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"e33d3552-2d74-457e-c65c-5cb1592a1984"},"source":["A_sum_axis0 = A.sum(axis=0)\n","A_sum_axis0, A_sum_axis0.shape"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([40., 45., 50., 55.]), torch.Size([4]))"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"fXpn-YIJ_Y64"},"source":["We specify axis=1 to reduce the column dimension & sum up column elements. "]},{"cell_type":"code","metadata":{"id":"bVwv6eLYUvdA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475401447,"user_tz":-120,"elapsed":525,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"3b9c6c9d-5cbe-4cf6-f133-b9ffc5c86fb2"},"source":["A_sum_axis1 = A.sum(axis=1)\n","A_sum_axis1, A_sum_axis1.shape"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"6kJGfrhw_haJ"},"source":["Sometimes it can be useful to keep the number of axes unchanged with keepdims parameter."]},{"cell_type":"code","metadata":{"id":"WxIcWg8lUyRb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475403572,"user_tz":-120,"elapsed":503,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"acd419f1-469a-4304-be80-7648175db985"},"source":["sum_A = A.sum(axis=1, keepdims=True)\n","sum_A"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 6.],\n","        [22.],\n","        [38.],\n","        [54.],\n","        [70.]])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"d3c9EMvoU3y6"},"source":["# Tensor Matrix Operations\n","\n","Most fundamental linear algebra operations is the dot product of two vectors.\n","\n","Dot product is a sum over the products of the elements at the same position in the two vectors.\n"]},{"cell_type":"code","metadata":{"id":"vyQkTlYrU7N-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475407296,"user_tz":-120,"elapsed":560,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"93a9cf12-f90e-432e-8b7a-2df38dfbc271"},"source":["x = torch.arange(4, dtype=torch.float32)\n","y = torch.ones(4, dtype=torch.float32)\n","x, y, torch.dot(x, y)"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"Fov6OoOs_7Kf"},"source":["Dot products only works for vectors. it will give an error for tensors of dimensions > 1\n","Instead we use mm function for matrix-matrix products:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4QYIbXdADco","executionInfo":{"status":"ok","timestamp":1619475409366,"user_tz":-120,"elapsed":503,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"14507c62-cfe8-44c0-b3b3-309719068573"},"source":["x = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n","y = torch.tensor([[5,6],[7,8]], dtype=torch.float32)\n","print(torch.mm(x, y))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["tensor([[19., 22.],\n","        [43., 50.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kkSTadI3ACrC"},"source":["To compute matrix-vector products we can use torch.mv; or we can use torch.matmul."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZil2dlPAL71","executionInfo":{"status":"ok","timestamp":1619475411763,"user_tz":-120,"elapsed":706,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"2f41bca8-8e03-4e41-dd48-7c7f549aff4b"},"source":["v = torch.tensor([9,10], dtype=torch.float32)\n","print(torch.mv(x, v))\n","print(torch.matmul(x, v))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["tensor([29., 67.])\n","tensor([29., 67.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MIjw5onEVCix"},"source":["# Tensor Norms\n","\n","In linear algebra, a vector norm is a function that maps a vector to a scalar.\n","The norm of a vector ￼ tells us how large is the magnitude of its components. \n","￼\n","L1 norm is the sum of the absolute values of the vector elements. \n","L2￼ norm is the square root of the sum of the squares of the vector elements.\n","\n","In deep learning, we often use squared ￼ norm to define our loss function."]},{"cell_type":"code","metadata":{"id":"ZvJ-0MeUVEbU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475414812,"user_tz":-120,"elapsed":511,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"a8da265b-675b-4a37-95b0-1ff598a4a860"},"source":["u = torch.tensor([3.0, -4.0])\n","torch.norm(u)"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(5.)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"8loFWgoMAjzm"},"source":["We also frequently use the  norm as it is less influenced by outliers."]},{"cell_type":"code","metadata":{"id":"yMy50PXmVHHJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475416906,"user_tz":-120,"elapsed":513,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"bd3fc94d-2287-41da-dbbb-82d5c690c6cb"},"source":["torch.abs(u).sum()"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(7.)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"RIIhK917VPXb"},"source":["# Tensor Memory-management"]},{"cell_type":"markdown","metadata":{"id":"nNoY5MUZAurK"},"source":["Running operations can cause new memory to be allocated to host results."]},{"cell_type":"code","metadata":{"id":"IgwUNeP5VRnS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475419053,"user_tz":-120,"elapsed":566,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"b78efd23-8441-4a1c-d7d7-ac6d9d4714a7"},"source":["before = id(Y)\n","Y = Y + X\n","id(Y) == before"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"4n5NMRZbAxzZ"},"source":["Running Y = Y + X, we will find that id(Y) points to a different location.\n","\n","In deep learning, we update millions of parameters multiple times per second.\n","\n","* Unnecessary memory allocation all the time. \n","* Can cause parts of our code to inadvertently reference stale parameters.\n","\n","We can assign the result of an operation to a previously allocated array with slice notation."]},{"cell_type":"code","metadata":{"id":"9RrEQgbQVUXk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475421839,"user_tz":-120,"elapsed":516,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"69c7d573-81a2-4ad2-ebef-9248c137a007"},"source":["before = id(X)\n","X[:] =  X + Y\n","id(X) == before"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"LzI39iA1BF6u"},"source":["If the value of X is not reused in subsequent computations, in-place operations reduce the memory overhead."]},{"cell_type":"markdown","metadata":{"id":"D_5jkS9lVXbj"},"source":["# Tensor Conversion\n","\n","Converting to a NumPy tensor, or vice versa."]},{"cell_type":"code","metadata":{"id":"sd-g5wzDVZaq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475426823,"user_tz":-120,"elapsed":546,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"24d85b7e-7bf5-4c4e-ee6c-c9c266afb4e3"},"source":["A = X.numpy()\n","B = torch.tensor(A)\n","type(A), type(B)"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(numpy.ndarray, torch.Tensor)"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"-DKWAkF9BdBV"},"source":["To convert a size-1 tensor to a Python scalar, we can invoke the item function or Python’s built-in functions."]},{"cell_type":"code","metadata":{"id":"o6n0dQddVbcu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619475428574,"user_tz":-120,"elapsed":540,"user":{"displayName":"Aseem behl","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTv2u8TeI-Fz7xTpD_sbxC_rlXCca-mWyYbxaQbHQ=s64","userId":"11994063867619304589"}},"outputId":"32bb4db2-3b1a-4fb3-a287-2b0c57f396df"},"source":["a = torch.tensor([3.5])\n","a, a.item(), float(a), int(a)"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([3.5000]), 3.5, 3.5, 3)"]},"metadata":{"tags":[]},"execution_count":45}]}]}